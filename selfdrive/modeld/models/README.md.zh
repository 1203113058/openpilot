# 神经网络模型

要查看ONNX网络的架构，可以使用[netron](https://netron.app/)

## 驾驶模型（视觉模型 + 时序策略模型）
### 视觉输入（总大小：799906 x float32）
* **图像流**
  * 以20Hz记录的两个连续图像（256 * 512 * 3 RGB格式）：393216 = 2 * 6 * 128 * 256
    * 每个256 * 512图像以YUV420格式表示，包含6个通道：6 * 128 * 256
      * 通道0,1,2,3代表全分辨率Y通道，在numpy中表示为Y[::2, ::2], Y[::2, 1::2], Y[1::2, ::2]和Y[1::2, 1::2]
      * 通道4代表半分辨率U通道
      * 通道5代表半分辨率V通道
* **广角图像流**
  * 以20Hz记录的两个连续图像（256 * 512 * 3 RGB格式）：393216 = 2 * 6 * 128 * 256
    * 每个256 * 512图像以YUV420格式表示，包含6个通道：6 * 128 * 256
      * 通道0,1,2,3代表全分辨率Y通道，在numpy中表示为Y[::2, ::2], Y[::2, 1::2], Y[1::2, ::2]和Y[1::2, 1::2]
      * 通道4代表半分辨率U通道
      * 通道5代表半分辨率V通道
### 策略输入
* **期望行为**
  * 一个热编码缓冲区，用于命令模型执行特定动作，需要发送过去5秒的数据（20FPS）：100 * 8
* **交通规则**
  * 一个热编码向量，告诉模型是靠右行驶还是靠左行驶：2
* **横向控制参数**
  * 速度和转向延迟，用于预测所需曲率：2
* **先前期望的曲率**
  * 先前预测的期望曲率向量：100 * 1
* **特征缓冲区**
  * 包含当前特征在内的中间特征缓冲区，形成5秒的时间上下文（20FPS）：100 * 512


### 驾驶模型输出格式（总大小：XXX x float32）
参考modeld中的**slice_outputs**和**parse_vision_outputs/parse_policy_outputs**。


## 驾驶员监控模型
* .onnx模型可以使用onnx运行时运行
* .dlc文件是预量化模型，仅在高通DSP上运行

### 输入格式
* 单一图像 W = 1440 H = 960 亮度通道（Y），来自平面YUV420格式：
  * 完整输入大小为1440 * 960 = 1382400
  * 归一化范围从0.0到1.0的float32（onnx运行器）或从0到255的uint8（snpe运行器）
* 相机校准角度（横滚、俯仰、偏航）来自liveCalibration：3 x float32输入

### 输出格式
* 84 x float32输出 = 2 + 41 * 2 ([解析示例](https://github.com/commaai/openpilot/blob/22ce4e17ba0d3bfcf37f8255a4dd1dc683fe0c38/selfdrive/modeld/models/dmonitoring.cc#L33))
  * 对于前排座位的每个人（2 * 41）
    * 面部姿势：12 = 6 + 6
      * 面部方向[俯仰、偏航、横滚]在相机坐标系中：3
      * 面部位置[dx, dy]相对于图像中心：2
      * 归一化面部大小：1
      * 以上输出的标准偏差：6
    * 面部可见概率：1
    * 眼睛：20 = (8 + 1) + (8 + 1) + 1 + 1
      * 眼睛位置和大小，及其标准偏差：8
      * 眼睛可见概率：1
      * 眼睛闭合概率：1
    * 佩戴太阳镜概率：1
    * 面部遮挡概率：1
    * 接触方向盘概率：1
    * 注意力集中概率：1
    * （已弃用）分心概率：2
    * 使用手机概率：1
    * 分心概率：1
  * 通用输出 2
    * 相机视觉不良概率：1
    * 左侧驾驶概率：1